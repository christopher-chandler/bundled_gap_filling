{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\nimport os\nimport kenlm\nimport math\nimport random\n\n#kenlm initialisieren\nmodel = kenlm.LanguageModel(\"lm/enwikidata.lm\")\n\n\n\n#Methode, die uns einen zufälligen seed sentence (als Liste) aussucht und mit ID zurückgibt\ndef choose_seed(target, corpus_list):\n    seed_id = random.randint(0, (len(corpus_list)-1))\n    seed_sentence = corpus_list[seed_id].copy()\n    return(seed_id, seed_sentence)\n\n\n\n#Schnittstelle zu Fastsubs\ndef fastsubs(sentence, target):\n    #Platzhaltermethode.\n    #Bekommt einen Satz mit markiertem Target als input und sucht Wörter, die das Target ersetzen könnten.\n    distractors = [\"digest\", \"drink\", \"medicine\"]\n    return(distractors)\n\n\n\n#diese Methode gibt uns mit kenlm den Score für ein gegebenes Trigramm zurück\ndef get_score(trigram):\n    score = model.score(trigram)\n    result = pow(10,score)\n    #Smoothing, da wir für viele Trigramme ein Ergebnis von 0 bekamen)\n    result = result + (1/10000000000)\n    return(result)\n\n\n\n#macht aus einem Input-Text (gegeben als Liste) Trigramme und berechnet deren Score. \ndef make_trigram(sentence):\n    #Leere Listen initialisieren\n    trigram_list = []\n    trigram_probs = []\n    n = 0\n    #Dann gehen wir unseren Input durch\n    while n < (len(sentence)-2):\n        trigram = sentence[n] + \" \" + sentence[n+1] + \" \" + sentence[n+2]\n        trigram_list.append(trigram)\n        trigram_probs.append(get_score(trigram))\n        n = n + 1\n    return(trigram_list, trigram_probs)\n\n\n\n#diese Methode bekommt als Input einen Satz (als Liste und mit einer markierten Stelle) und ein Wort\n#und gibt als Output die Wahrscheinlichkeit zurück, dass das Wort an dieser Stelle auftaucht\ndef prob_for_single(sentence, word):\n    modified_sen = sentence.copy()\n    i = 0\n    while i < len(sentence):\n        if sentence[i][0] == \"<\":\n            modified_sen[i] = word\n            break\n        i = i+1\n    trigram_list = make_trigram(modified_sen)[0]\n    trigram_probs = make_trigram(modified_sen)[1]\n    final_prob = 1\n    for prob in trigram_probs:\n        final_prob = final_prob * prob\n    return(final_prob)\n    \n\n\n#berechnet, mit welcher Wahrscheinlichkeit ein gegebenes Wort die \"Lösung\" für ein ganzes Bundle ist\ndef prob_for_bundle(bundle, word):\n    #wahrscheinlichkeit fängt bei 1 an und dann multiplizieren wir das einfach immer mit der neuen wahrscheinlichkeit\n    bundle_prob = 1\n    #die schleife geht alle sätze unseres bundles durch\n    for sentence in bundle:\n        #...und berechnet die wahrscheinlichkeit, mit der das wort in dieser lücke auftaucht\n        sentence_prob = prob_for_single(sentence, word)\n        #und dann multiplizieren wir das mit unserer aktuellen wahrscheinlichkeit des wortes für das bundle!\n        bundle_prob = bundle_prob * sentence_prob\n    return(bundle_prob)\n\n\n#diese Methode berechnet das aktuelle disambiguation level des Bundles (= wie viel wahrscheinlicher ist das target die lösung als ein distractor)\ndef disamb(bundle, target, distractor_list):\n    #Wahrscheinlichkeit des Targets für das aktuelle Bundle\n    target_prob = prob_for_bundle(bundle, target)\n    #Liste für die Wahrscheinlichkeiten der Distraktoren für das Bundle\n    distractors_prob = []\n    for distr in distractor_list:\n        distractors_prob.append(prob_for_bundle(bundle, distr))\n    #dann nehmen wir den größten wert aus dieser liste von wahrscheinlichkeiten (= die größte wahrscheinlichkeit, die nicht die des targets ist)\n    max_other_prob = max(distractors_prob)\n    #... und berechnen hiermit unser disambiguation level. das ist einfach nur log(wahrscheinlichkeit target/größte andere wahrscheinlichkeit)\n    if max_other_prob != 0:\n        disambiguation = math.log(target_prob/max_other_prob)\n        return(disambiguation)\n    else:\n        #Hier hatten wir, trotz des Smoothings, oft einen Error, bei dem wir für die Wahrscheinlichkeiten 0 bekamen. \n        #Aus Zeitgründen haben wir entschieden, hier dann stattdessen 1000 zurückzugeben - das wird später im Code dann zu einer Error-Nachricht gemacht. \n        return(1000)\n\n\n#Diese Methode prüft für alle Sätze in corpus_list, ob sie das disambiguation level optimieren, und gibt dann die ID des besten zurück\ndef best_next_sentence(corpus_list, bundle, target, distractor_list):\n    choosing_next_sen = []\n    for sentence in corpus_list:\n        new_bundle = bundle.copy()\n        new_bundle.append(sentence)\n        if disamb(new_bundle, target, distractor_list) == 1000:\n            #Falls wir kein disambiguation level berechnen konnten, überspringen wir dieses Item. \n            continue\n        else:\n            choosing_next_sen.append(disamb(new_bundle, target, distractor_list))\n    maximum = 0\n    i = 0\n    best_sentence_id = 0\n    while i < len(choosing_next_sen):\n        if choosing_next_sen[i] > maximum > 1000:\n            maximum = choosing_next_sen[i]\n            best_sentence_id = i\n        i = i+1\n    if best_sentence_id == 1000:\n        #Hier nochmal Sicherheitsvorkehrung: Wenn wir hier wieder ein fehlerhaftes disambiguation level haben, geben wir als Index -1 zurück\n        #Da das kein gültiger Listenindex ist, wird dann unten stattdessen ein Error geprintet\n        return(-1)\n    return(best_sentence_id)\n\n#entfernt die Target-Markierung aus der Liste (nötig für manche Tests)\ndef remove_marker(sentence):\n    i = 0\n    while i < len(sentence):\n        if sentence[i][0] == \"<\":\n            sentence[i] = sentence[i].replace(\"<\",\"\")\n            sentence[i] = sentence[i].replace(\">\",\"\")\n        i = i+1\n    return(sentence)\n\n#printet nochmal unsere komplette Aufgabe. \ndef output(bundle, target):\n    print(\"...\")\n    print(\"Hier ist die fertige Bundled Gap Filling Aufgabe:\")\n    print(\"\")\n    for sentence in bundle:\n        sentence = \"'\" + \" \".join(sentence) + \"'\"\n        print(sentence)\n\n\ndef main():\n    #Korpus, aus dem Sätze gezogen werden sollen, und Target festlegen sowie leeres Bundle initiieren\n    print(\"Initialisierung...\")\n    corpus = open(\"pandas2.txt\", \"r\")\n    target = \"eat\"\n    bundle = []\n    #Sätze, die das Target enthalten, als Liste in Liste schreiben\n    corpus_list = []\n    for line in corpus:\n        if target in line:\n            #Target markieren\n            line = line.replace(target, \"<\" + target + \">\")\n            list = line.split()\n            #... und als Liste an unsere Liste von in Frage kommenden Sätzen anhängen\n            corpus_list.append(list)\n    \n    #Seed sentence auswählen\n    seed_id, seed_sentence = choose_seed(target, corpus_list)\n    \n    #Satz zum Bundle hinzufügen\n    bundle.append(seed_sentence)\n    #Diesen Satz aus corpus_list entfernen (denn wir wollen ja keinen Satz doppelt im Bundle haben)\n    corpus_list.remove(corpus_list[seed_id])\n    \n    print(\"Generiere Distraktoren mit Fastsubs...\")\n    #Von Fastsubs distractors generieren lassen\n    distractor_list = fastsubs(seed_sentence, target)\n    \n    print(\"...\")\n    print(\"Unser erster Satz:\")\n    print(\"'\" + \" \".join(seed_sentence) + \"'\")\n    print(\"Disambiguation level: \" + str(disamb(bundle, target, distractor_list)))\n    \n    print(\"...\")\n    print(\"Jetzt fangen wir an, weitere Sätze zu suchen.\")\n    print(\"...\")\n    \n    #Hier suchen wir mithilfe einer while-Schleife immer den nächsten Satz aus, der unser disambiguation level optimiert. \n    count = 1\n    while count < 4:\n        best_sentence_id = best_next_sentence(corpus_list, bundle, target, distractor_list)\n        if best_sentence_id != -1:\n            bundle.append(corpus_list[best_sentence_id])\n            corpus_list.remove(corpus_list[best_sentence_id])\n            print(\"Satz Nr. \" + str(count+1) + \" gefunden!\")\n            print(\"Disambiguation level: \" + str(disamb(bundle, target, distractor_list)))\n        else:\n            bundle.append([\"ERROR: target_prob/max_other_prob = 0. Noch einmal versuchen?\"])\n            break\n        count = count+1\n    \n    #output\n    output(bundle, target)\n    \n\nif __name__ == \"__main__\":\n    \n    main()\n\n","metadata":{"trusted":true,"tags":[]},"execution_count":8,"outputs":[{"name":"stdout","text":"Initialisierung...\nGeneriere Distraktoren mit Fastsubs...\n...\nUnser erster Satz:\n'Every animal needs to <eat>.'\nDisambiguation level: 2.090243013842999\n...\nJetzt fangen wir an, weitere Sätze zu suchen.\n...\nSatz Nr. 2 gefunden!\nDisambiguation level: 3.6572261830516255\nSatz Nr. 3 gefunden!\nDisambiguation level: 6.267361322558341\nSatz Nr. 4 gefunden!\nDisambiguation level: 7.39109380550862\n...\nHier ist die fertige Bundled Gap Filling Aufgabe:\n\n'Every animal needs to <eat>.'\n'Pandas need to <eat>.'\n'Pandas love to <eat> bamboo the most.'\n'Today it is often a problem for the bears to find bamboo they can <eat>.'\n","output_type":"stream"},{"name":"stderr","text":"Loading the LM will be faster if you build a binary file.\nReading /home/jovyan/lm/enwikidata.lm\n----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n*The ARPA file is missing <unk>.  Substituting log10 probability -100.\n***************************************************************************************************\n","output_type":"stream"}],"id":"2d9966b4-e5fb-46b3-ab2b-2bfc48032ae3"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"60756a9d-30f3-4200-801c-05ab9f6b060a"}]}